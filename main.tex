\documentclass{mynotes}
\usepackage{mymacro}
\begin{document}
\tableofcontents
\chapter{Linear Transformations}
\section{Linear Transformations}
\begin{definition}[linear transformation]
let $V$ and $W$ be vector spaces over the field \F. A linear transformation from $V$ into $W$ is a function $T$ from $V$ into $W$ such that $$T(c\al+\be) = cT\al+T\be,$$ for all $\al,\be\in V$ and $c\in\F.$
\end{definition}
\begin{example}
Let $A$ be a fixed \mbyn matrix with entries in the field \F. The function $T$ defined by $T(x) = Ax$ is a linear transformation from $\F^{n\times1}$ to $\F^{m\times1}$. The function $U$ defined by $U(\al) = \al A$ is a linear transformation from $F^{1\times m}$ to $F^{1\times n}$
\end{example}
\begin{example}
Let \R{} be the field of real numbers and let $V$ be the space of all functions from \R{} to \R{} which are continuous. Define $T$ by $(Tf)(x) = \int_0^x f(t)\,\mathrm{d}t$. Then $T$ is a linear transformation from $V$ to $V$.
\end{example}
\begin{remark}
It's important to notice that if $T$ is a linear transformation from $V$ into $W$, then $$T(0_{{}_V}) = 0_{{}_W}.$$
\end{remark}
\begin{remark}
Linear transformation is actually defined to preserve linear combinations. That is 
$$T(c_1\al_1+c_2\al_2+\ldots + c_n\al_n) = c_1T(\al_1)+c_2T(\al_2)+\ldots + c_nT(\al_n)$$
\end{remark}
\begin{theorem}\label{t1}
Let $V$ be a finite dimensional vector space over the field \F, let $\{\al_1,\al_2,\ldots,\al_n\}$ be any ordered basis for $V$, let $W$ be a vector space over the same field \F{} and let $\be_1,\be_2,\ldots,\be_n$ be any vectors in $W$. Then there is precisely one linear transformation $T$ from $V$ into $W$ such that $T\al_j = \be_j$, for all $j=1,\ldots, n$.
\end{theorem}
\begin{proof}
Given $\al\in V$, there is a unique $n$-tuple $(x_1,x_2,\ldots,x_n)$ such that $$\al = x_1\al_1+x_2\al_2+\ldots+x_n\al_n.$$We define $T\al = x_1\be_1+x_2\be_2\ldots +x_n\be_n.$ Then $T$ is a well-defined rule for associating with each vector $\al\in V$ a vector $T\al\in W$. It's clear that $T\al_j = \be_j$ for each $j$ and $T$ is a linear transformation. \\
If $U$ is a linear transformation from $V$ into $W$ with $U\al_j=\be_j, j=1,2,\ldots,n$, then for the vector $\al = \sum_{i=1}^{n} x_i\al_i$ we have 
$$
U\al = U\left(\sum_{i=1}^nx_i\al_i\right)
	= \sum_{i=1}^nx_i\left(U\al_i\right)
	= \sum_{i=1}^n x_i\be_i.
$$So $U$ is exactly the same rule $T$ which we defined. This shows that the linear transformation $T$ with $T\al_j = \be_j$ for each $j$ is unique.
\end{proof}
\begin{remark}
The proof of Theorem \ref{t1} show us the way to actually get the transformation $T$. 
\end{remark}
\begin{problem}
The vector $\al_1 = (1,2)$, $\al_2 = (3, 4)$ form a basis for $\R^2$. $\be_1 = (3,2,1)$, $\be_2 = (6,5,4)$ are two vectors in $\R^3$. We now want to find a linear transformation $T$ such that $T\al_j=\be_j$.\\
We see that $(1,0) = -2(3,2,1) +(6,5,4)$, thus $T(1,0) = -2(3,2,1)+(6,5,4) = (0,1,2)$. Similarly, we can find $T(0,1)$. Then we know all about this $T$.
\end{problem}
\begin{example}\label{exF}
Let $T$ be a linear transformation from $\F^m$ to $\F^n$. By Theorem \ref{t1} we know that $T$ is uniquely determined by the sequence of vectors $\be_1,\be_2,\ldots,\be_m\in\F^n$ where $$\be_i = T\epsilon_i, \qquad i = 1,2,\ldots, m,$$Namely, if we have $$\al = (x_1,x_2,\ldots,x_m),$$then $$T\al = x_1\be_1+\ldots+x_n\be_n.$$If $B$ is the \mbyn{} matrix which has row vectors $\be_1,\be_2,\ldots,\be_m$, this says that $$T\alpha = \alpha B.$$
\end{example}
\begin{remark}
From Example \ref{exF} we show that we can give an explicit and reasonably simple description of all linear transformations from $\F^m$ to $\F^n$.
\end{remark}
\begin{remark}If $T$ is a linear transformatino from $V$ into $W$, then the range of $T$ is a subspace of $W$.\end{remark}
\begin{remark}The set $N$ consisting of the vectors $\al\in V$ such that $T\al = 0$ is a subspace of $V$.
\end{remark}
\begin{definition}[null space, rank, nullity]
Let $V$ and $W$ be vector spaces over the field \F{} and let $T$ be a linear transformation from $V$ to $W$. The null space of $T$ is the set of all vectors $\al\in V$ such that $T\al = 0.$
If $V$ is finite-dimensional, the rank of $T$ is the dimension of range of $T$ and the nullity of $T$ is the dimension of the null space of $T$.
\end{definition}
\begin{theorem}
Let $V$ and $W$ be vector spaces over the field \F{} and let $T$ be a linear transformation from $V$ into $W$. Suppose that $V$ is finite-dimensional, then $$\rank{T} + \nullity{T} = \dim V$$
\end{theorem}
\begin{proof}
Let $\{\al_1,\ldots,\al_k\}$ be a basis for the null space of $T$. We can find $\al_{k+1},\ldots,\al_{n}\in V$ such that $\{\al_1,\al_2,\ldots,\al_n\}$ is a basis for $V$. We shall show that $\{T\al_{k+1},\ldots,T\al_n\}$ is a basis for the range of $T$. The vector $T\al_1,\ldots,T\al_n$ certainly span the range of $T$. Since $T\al_1,\ldots,T\al_k$ are zero, $\{T\al_{k+1},\ldots,T\al_n\}$ spans the range of $T$. To see they are also independent, suppose we have scalars $c_i$ such that $$\sum_{i=k+1}^n c_i(T\al_i) = 0.$$Then we have $T(\sum_{i=k+1}^n c_i\al_i) = 0$ and accordingly $\al = \sum_{i = k+1}^n c_i\al_i$ is in the null space of $T$. We then must have $c_i = 0$ for $i = k+1,\ldots, n$. So $\{T\al_{k+1},\ldots,T\al_n\}$ is a basis for the range, we are done.
\end{proof}
\begin{theorem}
If $A$ is an \mbyn{} matrix with entries in the field \F, then $$\mbox{row rank}(A) = \mbox{column rank}(A).$$
\end{theorem}
\begin{proof}
Let $T$ be the linear transformation from $\F^{n\times1}$ to $\F^{m\times1}$ defined by $T(x)=Ax$. Say $S$ is the solution space of the system $Ax=0$, then $\nullity{T} = \dim{S}$. So we have $$\dim{S}+\rank{T} = n.$$ Notice that $\rank{T}$ is actually the dimension of the column space of matrix $A$. Say the RREF of matrix $A$ is $R$ and the row rank for $A$ and $R$ is $r$. Let $R_1,R_2,\ldots,R_n$ be the columns of matrix $R$, then there are $r$ of these columns, say $R_{p_1},\ldots,\R_{p_r}$, have a single $1$ as their only non-zero entry. Therefore, considering the space $S$, there are $n-r$ free variables, which means $\dim{S} = n-r$. So we have $\mbox{column rank}{A}=\mbox{row rank}{A}$.
\end{proof}
\section{The Algebra of Linear Transformation}
\begin{theorem}
Let $V$ and $W$ be vector spaces over the field \F. Let $T$ and $U$ be linear transformations from $V$ into $W$. The function $(T+U)$ defined by $$(T+U)(\al)=T\al+U\al$$is a linear transformation from $V$ into $W$. If $c$ is any element of \F, the function $(cT)$ defined by $(cT)(\al) = c(T\al)$ is a linear transformation from $V$ into $W$. The set of all linear transformation from $V$ into $W$, together with the addition and scalar multiplication defined above, is a vector space over the field \F.
\end{theorem}
\begin{proof}Omit.\end{proof}
\begin{remark}
We denote the space of linear transformations from $V$ into $W$ by $L(V,W)$.
\end{remark}
\begin{theorem}
Let $V$ be an $n$ dimensional vector space over the field \F, and let $W$ be an $m$ dimensional vector spaces over the field \F. Then the space $L(V,W)$ is finite-dimensional and has dimension $mn$.
\end{theorem}
\begin{proof}
Let $\mathcal{B} = \{\al_1,\al_2,\ldots,\al_n\}$ and $\mathcal{B}'= \{\be_1,\be_2,\ldots,\be_m\}$ be ordered bases for $V$ and $W$, respectively. For each pair of integer $(p,q)$ with $1\leq p\leq m$ and $1\leq q\leq n$, we define a linear transformation $E^{p,q}$ from $V$ into $W$ by $E^{p,q}(\al_i) = \delta_{iq}\be_p$.\\ Let $T$ be a linear transformation from $V$ to $W$. $(A_{1j},A_{2j},\ldots,A_{mj})$ is the coordinate vector of $T(\al_j)$ in the ordered basis $\mathcal{B}'$, i.e., $$T(\al_j) = \sum_{i=1}^mA_{ij}\be_i$$ Our claim is $$T=\sum_{p=1}^m\sum_{q=1}^nA_{pq}E^{p,q}.$$Actually, Let $U$ be the linear transformation defined by RHS of the equation, then for each $j$, 
\begin{align*}U(\al_j) &= \sum_{p=1}^m\sum_{q=1}^nA_{pq}E^{p,q}\al_j\\ &= \sum_{p=1}^m\sum_{q=1}^nA_{pq}\delta_{jq}\be_p\\ &=\sum_{p=1}^mA_{pj}\be_p = T(\al_j).\end{align*}So $T=U.$ This means $\{E^{p,q}\}$ spans $L(V,W)$. Furthermore, $\{E^{p,q}\}$ are independent because if $T=\sum\limits_{p}\sum\limits_{q}A_{pq}E^{p,q}$ is the zero transformation, then for each $j$, $T(\al_j)= \sum\limits_iA_{ij}\be_i=0$. So $A_{ij}=0$ for every $i,j$.
\end{proof}
\begin{theorem}\label{costheorem}
Let $V,W,Z$ be vector spaces over the field \F. Let $U$ be a  linear transoformation from $V$ into $W$, and $T$ be a linear transformation from $W$ to $Z$, then the funtion $(TU)$ defined by $(TU)(\al) = T(U{(\al)})$ is a linear transformation from $V$
 to $Z$.
 \end{theorem}
\begin{proof}Omit.\end{proof}
\begin{definition}[linear operator]
Let $V$ be a vector spaces over the field \F. A linear operator on $V$ is a linear transformation from $V$ into $V$.
\end{definition}
\begin{remark}
We notice that in Theorem \ref{costheorem}, if $V=W=Z$, then $(TU)$ is also a linear operator on $V$, i.e., there is a `multiplication' operation defined by composition on $L(V,V)$. In addition, $(UT)$ is also defined, but in general $(UT)-(TU)$ is not zero transformation.
\end{remark}
\begin{remark}
If $T$ is a linear operator on $V$, then we can define $T^n = TTT\ldots T$ without confusion. Proof is omitted. For convenience, we define $T^0=I\mbox{(identity transformation)}.$
\end{remark}
\begin{lemma}\label{proAl}
Let $V$ be a vector space over the field \F, $U,T_1,T_2$ be linear operators on $V$, c is any elements in the field \F.
\begin{itemize}
	\item[1)] $U=UI=IU$
	\item[2)] $U(T_1+T_2) = UT_1+UT_2;(T_1+T_2)U=T_1U+T_2U$
	\item[3)] $c(UT) = (cU)T=U(cT)$
\end{itemize}
\end{lemma}
\begin{remark}
Lemma \ref{proAl} and Theorem \ref{costheorem} tell us that $L(V,V)$, together with cosposion, is what known as a linear algebra with identity.
\end{remark}
\begin{example}
Let $A$ be an \mbyn matrix and $T$ be a linear transformation defined by $T(X) = Ax$. Let $B$ be an $p\times m$ matrix and $U$ be a linear transformation defined by $U(Y) = BY$. Then \begin{align*}(UT)(X) &= U(T(X))\\&=U(AX)\\&=B(AX)=BAX.\end{align*}
\end{example}
\begin{remark}
The effect of cosposition of $U,T$ is multiplication of matrices $B,A$.
\end{remark}
\begin{definition}[invertible]\label{invdef}
A linear transformation $T$ from $V$ into $W$ is invertible if there exist a function $U$ from $W$ into $V$ such that $(UT)$ is the identity transformation on $V$ and $(TU)$ is the identity transformaton on $W$. In this case, $U$ is unique and we denote $U$ by $T^{-1}$.
\end{definition}
\begin{remark}
In Definition \ref{invdef}, $T^{-1}$ exists if and only if
\begin{itemize}
	\item[1.] $T$ is one-one. ($T\al=T\be\implies \al=\be$)
	\item[2.] $T$ is onto. (The range of $T$ is $W$)
\end{itemize}
\end{remark}
\begin{theorem}
let $T$ be a linear transformation from $V$ into $W$. If $T$ is invertible, then the inverse $T^{-1}$ is a linear transformation from $W$ into $V$.
\end{theorem}
\begin{proof}
Omit.
\end{proof}
\begin{remark}
We see that $T^{-1}U^{-1}$ is the left and right inverse of $UT$, therefore the inverse of $(UT)$ is $T^{-1}U^{-1}$.
\end{remark}
\begin{definition}[non-singular]
We call a linear transformation $T$ non-singular if $T\gamma=0\implies\gamma=0,$i.e., the null space of $T$ is $0$.
\end{definition}
\begin{remark}
Evidently, $T$ is one-one if and only if $T$ is non-singular.
\end{remark}
\begin{remark}
 Non-singular linear transformations are those which preserve linear independence, as the following theorem claims.
\end{remark}
\begin{theorem}
Let $T$ be a linear transformation from $V$ into $W$. Then $T$ is non-singular if and only if $T$ carries each linearly independent subset of $V$ onto a linearly independent subset of $W$.
\end{theorem}
\begin{proof}
Suppose that $T$ is non-singular. Let $S$ be a linearly independent subset of $V$. If $\al_1,\ldots,\al_k$ are vectors in $S$, then the vector $T\al_1,\ldots,T\al_k$ are linearly independent. For if $$c_1(T\al_1)+\ldots+c_k(T\al_k)=0$$then $$T(c_1\al_1+\ldots+c_k\al_k) = 0$$therefore $$c_1\al_1+\ldots+c_k\al_k = 0.$$Since $\al_i$ are linearly independent, we have for each $j=1,2,\ldots,k$, $c_j = 0$.\\
Suppose that $T$ carries independent subsets onto independent subsets. Then $T$ must be non-singular. For if $T\al=0$, and $\al$ is not $0$. Then an independent set $S$ consisting of $\al$ will have its image a dependent set.
\end{proof}
\begin{theorem}
Let $V$ and $W$ be finite-dimensional vector spaces over the field \F{} such that $\dim{V}=\dim{W}$. If $T$ is a linear transformation from $V$ into $W$, the following are equivalent:
\begin{itemize}
	\item[(i)]	$T$ is invertible.
	\item[(ii)] $T$ is non-singular.
	\item[(iii)] $T$ is on-to.
\end{itemize}
\end{theorem}
\begin{proof}
Let $n=\dim{V}=\dim{W}.$ Since $$\rank{T}+\nullity{T} = n.$$So if $T$ is non-singular, then $\nullity{T} = 0$ and $\rank{T}= n$, i.e., $T$ is on-to.
If $T$ is on-to, then $\rank{T} = n$ and therefore $\nullity{T} = 0$ ($T$ is non-singular). \\Therefore $T$ is non-singular if and only if $T(V)=W.$ So, if either condition (ii) or (iii) holds, the other is satisfied as well and $T$ is invertible.
\end{proof}
\section{Isomorphism}
\begin{definition}[isomorphism]
If $V$ and $W$ are vector spaces over the field \F, any one-one linear transformation $T$ from $V$ onto $W$ is called an isomorphism of $V$ onto $W$. If there exists an isomorphism of $V$ onto $W$, we say that $V$ is isomorphic to $W$.
\end{definition}
\begin{remark}
If $V$ is isomorphic to $W$, then $W$ is isomorphic to $V$.
\end{remark}
\begin{theorem}
Every $n$-dimensional vector space over the field \F is isomorphic to the space $\F^n.$
\end{theorem}
\begin{proof}
Let $V$ be an $n$-dimensional vector space over the field \F and let $\mathcal{B}=\{\al_1,\ldots,\al_n\}$ be a basis for $V$. We define a function $T$ from $V$ to $\F^n$, as follows: If $\al$ is in $V$, let $T\al$ be the $n$-tuple $(x_1,\ldots,x_n)$ of coordinates of $\al$ relative to the ordered basis $\mathcal{\be}$. Also, it's easy to verify $T$ is a linear transformation and $T$ is one-one and on-to.
\end{proof}

























\end{document}